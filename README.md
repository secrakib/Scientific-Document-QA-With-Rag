# ğŸ”¬ Context-Aware Scientific Document Q/A System  
*A Retrieval-Augmented Generation (RAG) system for interactive scientific document exploration.*

[![Live Demo](https://img.shields.io/badge/Live-Demo-brightgreen)](https://paperchat-frontend.onrender.com/)


<!-- Optional: Add a preview GIF or screenshot -->


[![Python](https://img.shields.io/badge/Python-3.12+-blue.svg)](https://www.python.org/)

---

## ğŸ“˜ Overview

This project enables **natural language interaction with scientific PDFs**, allowing users to query documents conversationally and receive **context-aware, memory-driven** responses.

By combining **RAG techniques, Gemini embeddings, and FAISS semantic search**, the system delivers accurate, contextually grounded answers while maintaining conversational memory across interactions.

---

## ğŸš€ Key Features

- ğŸ§  **Context-Aware Q/A** â€” Understands user queries in the context of prior conversation.  
- ğŸ” **Semantic Search with FAISS** â€” Retrieves the most relevant document chunks efficiently.  
- ğŸ”— **RAG Pipeline (LangChain)** â€” Connects retrieval, document, and memory chains for precise, dynamic reasoning.  
- ğŸ’¬ **Interactive UI** â€” Built with Streamlit for seamless document upload and exploration.  
- âœ… **Tested & Modular Design** â€” Comprehensive unit tests across all core modules.  

---

## ğŸ§© Processing Pipeline

```
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚   User / Frontend â”‚
                          â”‚   (Streamlit UI) â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚ Upload PDF / Ask Query
                                    â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚     Backend       â”‚
                          â”‚   (FastAPI API)   â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                         â”‚                         â”‚
          â–¼                         â–¼                         â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ PDF Upload      â”‚       â”‚ Query / Session â”‚       â”‚ Session Memory  â”‚
 â”‚ (uploads/)      â”‚       â”‚ Validation      â”‚       â”‚ (memory_chain)  â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                         â”‚                         â”‚
           â–¼                         â–¼                         â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ PDF Loader      â”‚       â”‚ Text Splitter    â”‚       â”‚ Retriever        â”‚
 â”‚ data_loader()   â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚ text_splitter() â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚ retriver()       â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                         â”‚                         â”‚
           â–¼                         â–¼                         â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Metadata        â”‚       â”‚ Embeddings       â”‚       â”‚ FAISS Vector DB  â”‚
 â”‚ ingestion       â”‚       â”‚ gemini_embedding â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚ vector_database â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                         â”‚                         â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼                        â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ Document / Retrieval     â”‚
                  â”‚ Chains (RAG Pipeline)    â”‚
                  â”‚ document_chain(),        â”‚
                  â”‚ retrival_chain(),        â”‚
                  â”‚ memory_chain()           â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ LLM Response             â”‚
                  â”‚ llm() generates answer   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ Frontend / User UI       â”‚
                  â”‚ Displays Answer          â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```


---

## âš™ï¸ Tech Stack

| Component | Technology |
|------------|-------------|
| **LLM Framework** | LangChain |
| **Embeddings** | Gemini |
| **Vector Store** | FAISS |
| **Backend API** | FastAPI |
| **Frontend** | Streamlit |
| **Deployment** | Render |

---

## ğŸ§  How It Works

1. **Upload a PDF** â†’ The document is split into semantically meaningful chunks.  
2. **Embedding Generation** â†’ Each chunk is embedded using **Gemini embeddings**.  
3. **FAISS Indexing** â†’ Chunks are stored and retrieved efficiently via **FAISS**.  
4. **RAG Processing** â†’ A **LangChain pipeline** retrieves relevant chunks and generates responses.  
5. **Conversational Memory** â†’ User history is retained for context-aware answers.

---

## ğŸ§‘â€ğŸ’» Setup & Installation

### Clone and Run the Project
```
Clone the repo

git clone https://github.com/yourusername/context-aware-scientific-qa.git
cd context-aware-scientific-qa

Create a Virtual Environment

python -m venv venv
source venv/bin/activate     # On macOS/Linux
venv\Scripts\activate        # On Windows

Install Dependencies
pip install -r requirements.txt

4. Run the Streamlit App
streamlit run app.py
```

### ğŸ”§ Environment Variables
This project requires a .env file in the root directory to store environment-specific configuration values.
Create a file named .env in the project root with the following content:
```
# .env
GOOGLE_API_KEY=your_google_api_key_here
```

### ğŸ“š Example Query
```
User: â€œWhat is the main contribution of this paper?â€
System: â€œThe paper proposes a context-aware retrieval model for improved document-level question answering, leveraging multi-hop reasoning across sections.â€
```

### ğŸ§± Project Structure
```
â”œâ”€ .vscode/
â”‚
â”œâ”€ backend/
â”‚  â”œâ”€ api/
â”‚  â”‚  â”œâ”€ uploads/
â”‚  â”‚  â”œâ”€ main.py
â”‚  â”‚  â””â”€ __init__.py
â”‚  â”‚
â”‚  â”œâ”€ chains/
â”‚  â”œâ”€ embeddings/
â”‚  â”œâ”€ ingestion/
â”‚  â”œâ”€ llm/
â”‚  â”œâ”€ retriever/
â”‚  â”œâ”€ splitting/
â”‚  â”œâ”€ vector_database/
â”‚  â”œâ”€ Dockerfile
â”‚  â”œâ”€ __init__.py
â”‚  â””â”€ requirements.txt
â”‚
â”œâ”€ frontend/
â”‚  â”œâ”€ Dockerfile
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ app.py
â”‚  â””â”€ requirements.txt
â”‚
â”œâ”€ tests/
â”‚
â”œâ”€ uploads/
â”‚
â”œâ”€ .gitignore
â”œâ”€ LICENSE
â”œâ”€ README.md
â”œâ”€ __init__.py
â”œâ”€ docker-compose.yml
â””â”€ requirements.txt

```
### âš ï¸ Known Issues / Limitations
```
The system currently requires a Google API key to function; other API providers are not yet supported.

The embedding process may be slow for very large documents.

Frontend performance can degrade with large datasets.

Docker setup assumes a Unix-like environment; Windows users may need additional configuration.

Error handling and logging are minimal and should be improved for production use.

```
### ğŸ“ˆ Future Improvements
```
ğŸ”„ Support for multi-document context

ğŸ—£ï¸ Voice query integration

ğŸ§© Improved memory persistence with vectorized history

ğŸŒ Support for multilingual scientific texts
```
### 

### ğŸ“„ License
```
This project is licensed under the MIT License
```
